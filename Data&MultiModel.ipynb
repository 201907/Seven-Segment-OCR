{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamomeni/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Models.py, line 153)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/aamomeni/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-07bbe365d200>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from Models import ModelMulti\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/aamomeni/Seven-Segment-OCR/Models.py\"\u001b[0;36m, line \u001b[0;32m153\u001b[0m\n\u001b[0;31m    def __init__(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from Models import ModelMulti\n",
    "import tensorflow as tf\n",
    "\n",
    "session_config = tf.ConfigProto()\n",
    "session_config.gpu_options.visible_device_list = \"0\"\n",
    "session_config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=session_config))\n",
    "\n",
    "model_1 = Model_Multi()\n",
    "model_1.train_predict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate LQ, MQ, HQ\n",
    "\n",
    "suffix = \".csv\"\n",
    "csv_directory = \"Datasets/\"\n",
    "csv_files = [i for i in os.listdir(csv_directory) if i.endswith( suffix )]\n",
    "\n",
    "full_data = []\n",
    "for i in range(len(csv_files)):\n",
    "    data = pd.read_csv(csv_directory+'/'+csv_files[i], sep=';', index_col = 0)\n",
    "    full_data.append(data)\n",
    "\n",
    "full_data = pd.concat(full_data, axis=0)\n",
    "print(full_data.shape)\n",
    "full_data.head(25)\n",
    "\n",
    "\n",
    "\n",
    "full_data =  pd.read_csv('Datasets/HQ_digital.csv', sep=';', index_col = 0)\n",
    "full_data = full_data.replace(\"X\", 10)\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Only use data for which we have images\n",
    "\n",
    "frame_directory = \"Datasets_frames/\"\n",
    "\n",
    "data = full_data[full_data[\"image\"].isin(os.listdir(frame_directory))]\n",
    "print(data.shape)\n",
    "data.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into test and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = data.iloc[:,1]\n",
    "y = data.iloc[:,2:]\n",
    "\n",
    "ids_train, ids_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vect = [y_train[\"cadran_1\"], y_train[\"cadran_2\"], y_train[\"cadran_3\"], y_train[\"cadran_4\"]]\n",
    "y_val_vect =  [y_val[\"cadran_1\"], y_val[\"cadran_2\"], y_val[\"cadran_3\"], y_val[\"cadran_4\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_arrays(samples, directory):\n",
    "    X = []\n",
    "    for sample in samples:\n",
    "        ID =  directory + \"%s\" % (sample)\n",
    "        img = Image.open(ID)\n",
    "        img = np.array(img)\n",
    "        img = cv2.equalizeHist(img)\n",
    "        img = cv2.threshold(img, 40, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "        img = cv2.dilate(img, None, iterations=1)\n",
    "        img = img[:,:245]\n",
    "        img = img.reshape((img.shape[0],img.shape[1],1))\n",
    "        X.append(img)\n",
    "    X = np.asarray(X)\n",
    "    return X\n",
    "\n",
    "X_train = convert_to_arrays(ids_train, frame_directory)\n",
    "X_val = convert_to_arrays(ids_val, frame_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import regularizers\n",
    "import keras.backend\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "session_config = tf.ConfigProto()\n",
    "session_config.gpu_options.visible_device_list = \"0\"\n",
    "session_config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=session_config))\n",
    "\n",
    "\n",
    "model_input = Input((100,245,1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), padding='same', name='conv2d_hidden_1', kernel_regularizer=regularizers.l2(0.01))(model_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_1')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', name='conv2d_hidden_2', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_2')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same', name='conv2d_hidden_3', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_3')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256, activation ='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "digit1 = (Dense(output_dim =11,activation = 'softmax', name='digit_1'))(x)\n",
    "digit2 = (Dense(output_dim =11,activation = 'softmax', name='digit_2'))(x)\n",
    "digit3 = (Dense(output_dim =11,activation = 'softmax', name='digit_3'))(x)\n",
    "digit4 = (Dense(output_dim =11,activation = 'softmax', name='digit_4'))(x)\n",
    "\n",
    "outputs = [digit1, digit2, digit3, digit4]\n",
    "\n",
    "M = Model(input = model_input , output = outputs)\n",
    "M._make_predict_function()\n",
    "\n",
    "opt = Adam(lr=0.001, decay=0.0001)\n",
    "\n",
    "M.compile(loss=\"sparse_categorical_crossentropy\", optimizer= opt, metrics = ['accuracy'])\n",
    "\n",
    "keras.backend.get_session().run(tf.initialize_all_variables())\n",
    "\n",
    "history =M.fit(X_train, y_train_vect, batch_size= 50, nb_epoch=10, verbose=1, validation_data=(X_val, y_val_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc Curves\n",
    "\n",
    "for i in range(1,5):\n",
    "        plt.figure(figsize=[8,6])\n",
    "        plt.plot(history.history['digit_%i_acc' %i],'r',linewidth=0.5)\n",
    "        plt.plot(history.history['val_digit_%i_acc' %i],'b',linewidth=0.5)\n",
    "        plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "        plt.xlabel('Epochs ',fontsize=16)\n",
    "        plt.ylabel('Accuracy',fontsize=16)\n",
    "        plt.title('Accuracy Curves Digit %i' %i,fontsize=16)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['digit_2_loss'],'r',linewidth=0.5)\n",
    "plt.plot(history.history['val_digit_2_loss'],'b',linewidth=0.5)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 'Datasets_frames/08d21828339e1a6adcd892ef2f0d0866083c51bf.jpg'\n",
    "img = Image.open(ID)\n",
    "img = np.array(img)\n",
    "img = cv2.equalizeHist(img)\n",
    "img = cv2.threshold(img, 40, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "img = cv2.dilate(img, None, iterations=1)\n",
    "img = img[:,:245]\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape((1,img.shape[0],img.shape[1],1))\n",
    "preds = M.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    print(np.argmax(preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = M.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds = 0\n",
    "\n",
    "for i in range(X_val.shape[0]):         \n",
    "    pred_list_i = [np.argmax(pred[i]) for pred in y_pred]\n",
    "    val_list_i  = y_val.values[i].astype('int')\n",
    "    if np.array_equal(val_list_i, pred_list_i):\n",
    "        correct_preds = correct_preds + 1\n",
    "\n",
    "        \n",
    "mse = 0 \n",
    "diff = []\n",
    "\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "        pred_list_i = [np.argmax(pred[i]) for pred in y_pred]\n",
    "        pred_number = 1000* pred_list_i[0] + 100* pred_list_i[1] + 10 * pred_list_i[2] + 1* pred_list_i[3]\n",
    "        val_list_i  = y_val.values[i].astype('int')\n",
    "        val_number = 1000* val_list_i[0] + 100*  val_list_i[1] + 10 *  val_list_i[2] + 1*  val_list_i[3]\n",
    "        diff.append(val_number - pred_number)\n",
    "        \n",
    "\n",
    "np.sort(diff)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list_i  = [val[i] for val in y_val]\n",
    "    matching_preds = [pred.argmax(-1) == val.argmax(-1) for pred, val in zip(pred_list_i, val_list_i)]\n",
    "    correct_preds = int(np.all(matching_preds))\n",
    "\n",
    "total_acc = (correct_preds / float(X_test.shape[0]))*100\n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "labels = []\n",
    "for i in range(11):\n",
    "    directory = 'Datasets_digits/%i/' %i\n",
    "    for j in os.listdir(directory):\n",
    "        if j != '.DS_S':\n",
    "            ids.append(directory+j)\n",
    "            labels.append(i)\n",
    "\n",
    "data = pd.DataFrame(list(zip(ids,labels)))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    directory = 'Datasets_Eleven/%i/' %i\n",
    "    for j in os.listdir(directory):\n",
    "        ids.append(directory+j)\n",
    "        labels.append(i)\n",
    "data = pd.DataFrame(list(zip(ids,labels)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0]\n",
    "y = data.iloc[:,1]\n",
    "\n",
    "ids_train, ids_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "def convert_digits_to_arrays(samples):\n",
    "    X = []\n",
    "    for sample in samples:\n",
    "        img = Image.open(sample)\n",
    "        img = np.array(img)\n",
    "        #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img.resize((30,50))\n",
    "        img = cv2.equalizeHist(img)\n",
    "        img = cv2.threshold(img, 40, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "        img = cv2.dilate(img, None, iterations=1)\n",
    "        img = np.array(img)\n",
    "        img = img.reshape((30,50,1))\n",
    "        X.append(img)\n",
    "    X = np.asarray(X)\n",
    "    return X\n",
    "\n",
    "X_train = convert_digits_to_arrays(ids_train)\n",
    "X_val = convert_digits_to_arrays(ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input((30,50,1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), padding='same', name='conv2d_hidden_1', kernel_regularizer=regularizers.l2(0.01))(model_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_1')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Conv2D(63, (3, 3), padding='same', name='conv2d_hidden_2', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_2')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same', name='conv2d_hidden_3', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_3')(x)\n",
    "x = Dropout(0.30)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(1024, activation ='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "output = Dense(output_dim =11,activation = 'softmax', name='output')(x)\n",
    "\n",
    "M = Model(input = model_input , output = output)\n",
    "\n",
    "opt = Adam(lr=0.001)\n",
    "\n",
    "M.compile(loss=\"sparse_categorical_crossentropy\", optimizer= opt, metrics = ['accuracy'])\n",
    "\n",
    "history = M.fit(X_train, y_train, batch_size= 32, nb_epoch=30, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = M.predict(X_val)\n",
    "\n",
    "e = []\n",
    "for i in range(X_val.shape[0]):\n",
    "    val_id = ids_val.values[i]\n",
    "    pred_list_i = np.argmax(y_pred[i]).astype('int')\n",
    "    val_list_i  = y_val.values[i].astype('int')\n",
    "    if val_list_i != pred_list_i:\n",
    "        e.append(val_id.split('/')[2].split('-')[0])\n",
    "np.unique(e)\n",
    "#        print(val_id, pred_list_i, val_list_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_id.split('/')[2].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
